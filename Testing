for file_path in log_files:
    print(f"--> Processing: {file_path}")
    
    try:
        # Read file as RDD to handle line numbers
        rdd = spark.sparkContext.textFile(file_path)
        rdd_indexed = rdd.zipWithIndex()
        
        # ---------------------------------------------------------
        # A. EXTRACT HEADER (Row 2 -> Index 1)
        # ---------------------------------------------------------
        header_rdd = rdd_indexed.filter(lambda x: x[1] == 1).map(lambda x: x[0])
        
        if header_rdd.isEmpty():
            print("    Skipping: File too short.")
            continue
            
        raw_header_str = header_rdd.first() # e.g., "fields: 'col1', 'col2'"
        
        # Clean the "fields:" prefix
        if ':' in raw_header_str:
            clean_header_str = raw_header_str.split(':', 1)[1].strip()
        else:
            clean_header_str = raw_header_str.strip()
            
        # Parse Header using Spark (Ensures consistency with Data parsing)
        temp_header_rdd = spark.sparkContext.parallelize([clean_header_str])
        df_header_parsed = spark.read.option("header", "false") \
                                     .option("quote", "'") \
                                     .option("escape", "\\") \
                                     .option("ignoreLeadingWhiteSpace", "true") \
                                     .csv(temp_header_rdd)
                                     
        columns = [c.strip() for c in df_header_parsed.head()]
        print(f"    Detected {len(columns)} columns.")

        # ---------------------------------------------------------
        # B. EXTRACT DATA (Row 4+ -> Index > 2)  <-- FIXED HERE
        # ---------------------------------------------------------
        # We skip Index 0, 1, and 2. Data starts at Index 3.
        data_rdd = rdd_indexed.filter(lambda x: x[1] > 2).map(lambda x: x[0])
        
        df_data = spark.read.option("header", "false") \
                            .option("quote", "'") \
                            .option("escape", "\\") \
                            .option("ignoreLeadingWhiteSpace", "true") \
                            .csv(data_rdd)

        # ---------------------------------------------------------
        # C. VALIDATE & WRITE
        # ---------------------------------------------------------
        # Check if data exists
        if len(df_data.head(1)) == 0:
            print("    Warning: Header found, but no data rows (starting row 4).")
            continue

        if len(df_data.columns) == len(columns):
            # Apply columns
            df_final = df_data.toDF(*columns)
            
            # Add metadata
            df_final = df_final.withColumn("source_filename", F.lit(file_path.split('/')[-1])) \
                               .withColumn("ingestion_date", F.current_timestamp())

            # Write to Delta
            df_final.write.format("delta") \
                .mode("append") \
                .option("mergeSchema", "true") \
                .save(dest_delta_path)
                
            print(f"    Success: Appended {df_final.count()} rows.")
            
        else:
            print(f"    ERROR: Mismatch! Header has {len(columns)} cols, Data has {len(df_data.columns)} cols.")
            print(f"    Sample Data Row: {df_data.head()}")

    except Exception as e:
        print(f"    CRITICAL FAIL: {e}")
