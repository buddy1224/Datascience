for file_path in log_files:
    print(f"--> Processing: {file_path}")
    
    try:
        # A. Read file as an RDD (Resilient Distributed Dataset) to handle line numbers
        rdd = spark.sparkContext.textFile(file_path)
        
        # Check if file has enough lines (needs at least 2 lines for header)
        if rdd.take(2) == []: 
            print("    Skipping: File is empty or too short.")
            continue

        # B. Extract the Header (Line 2 / Index 1)
        # We filter for index 1 and collect it to the driver
        header_line = rdd.zipWithIndex().filter(lambda x: x[1] == 1).map(lambda x: x[0]).collect()
        
        if not header_line:
            print("    Skipping: Could not find header line.")
            continue
            
        # Clean header: split by comma and strip whitespace
        # Note: We replace spaces in column names to avoid Spark errors
        columns = [c.strip().replace(" ", "_") for c in header_line[0].split(',')]

        # C. Extract Data (Lines > 2)
        # We filter RDD to keep only lines after the header
        data_rdd = rdd.zipWithIndex().filter(lambda x: x[1] > 1).map(lambda x: x[0])

        # D. Convert to DataFrame
        # We use spark.read.csv on the RDD to handle CSV parsing logic (quotes, escapes) correctly
        df_data = spark.read.option("header", "false") \
                            .option("inferSchema", "true") \
                            .csv(data_rdd)

        # Apply the column names we extracted
        # If the CSV row has more columns than the header, we slice or handle gracefully
        if len(df_data.columns) == len(columns):
            df_final = df_data.toDF(*columns)
        else:
            # Fallback for mismatched columns: Keep auto-names (_c0, _c1) and print warning
            print(f"    Warning: Column count mismatch. Header: {len(columns)}, Data: {len(df_data.columns)}")
            df_final = df_data

        # Add Source Filename for tracking (ETL Best Practice)
        df_final = df_final.withColumn("source_filename", F.lit(file_path.split('/')[-1])) \
                           .withColumn("ingestion_date", F.current_timestamp())

        # E. Write to Delta Lake with Schema Merging
        df_final.write.format("delta") \
            .mode("append") \
            .option("mergeSchema", "true") \
            .save(dest_delta_path)

        processed_count += 1
        print(f"    Success: Appended {df_final.count()} rows.")

    except Exception as e:
        print(f"    FAILED: {str(e)}")

print(f"------------------------------------------------")
print(f"Process Complete. {processed_count} files merged.")
